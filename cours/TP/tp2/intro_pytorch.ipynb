{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à PyTorch : Du Scalaire au Tenseur\n",
    "\n",
    "Dans ce notebook, vous allez découvrir les concepts fondamentaux de PyTorch :\n",
    "1. **Tenseurs** : Les briques de base (équivalent aux arrays NumPy avec GPU)\n",
    "2. **Autograd** : Différentiation automatique\n",
    "3. **nn.Module** : Construction de réseaux de neurones\n",
    "4. **Optimisation** : Boucle d'entraînement\n",
    "\n",
    "## Contexte\n",
    "\n",
    "Ce matin, vous avez implémenté micrograd avec des scalaires. Maintenant, nous passons à l'échelle avec des **tenseurs** pour exploiter le parallélisme du GPU."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vérifier si GPU disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Les Tenseurs\n",
    "\n",
    "Un tenseur est une généralisation des matrices à N dimensions :\n",
    "- **Scalaire** : tenseur 0D (un nombre)\n",
    "- **Vecteur** : tenseur 1D \n",
    "- **Matrice** : tenseur 2D\n",
    "- **Tenseur 3D+** : images, vidéos, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Création de tenseurs\n",
    "scalar = torch.tensor(3.14)\n",
    "vector = torch.tensor([1, 2, 3, 4, 5])\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(f\"Scalaire: shape={scalar.shape}, valeur={scalar.item()}\")\n",
    "print(f\"Vecteur: shape={vector.shape}\")\n",
    "print(f\"Matrice: shape={matrix.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Tenseurs spéciaux\n",
    "zeros = torch.zeros(3, 4)       # Matrice 3x4 de zéros\n",
    "ones = torch.ones(2, 3, 4)      # Tenseur 3D de uns\n",
    "random = torch.randn(5, 5)      # Distribution normale\n",
    "identity = torch.eye(4)         # Matrice identité\n",
    "\n",
    "print(f\"zeros: {zeros.shape}\")\n",
    "print(f\"ones: {ones.shape}\")\n",
    "print(f\"randn: {random.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.1 : Créer des tenseurs\n",
    "\n",
    "Créez les tenseurs suivants :\n",
    "1. Un tenseur de forme (32, 10) rempli de valeurs aléatoires uniformes entre 0 et 1\n",
    "2. Un tenseur représentant un batch de 16 images RGB de 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Exercice 1.1\n",
    "# uniform_tensor = ???\n",
    "# image_batch = ???\n",
    "\n",
    "# Vérification\n",
    "# assert uniform_tensor.shape == (32, 10)\n",
    "# assert image_batch.shape == (16, 3, 28, 28)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Opérations sur les Tenseurs\n",
    "\n",
    "PyTorch supporte toutes les opérations mathématiques classiques."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Opérations élément par élément\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.tensor([4.0, 5.0, 6.0])\n",
    "\n",
    "print(f\"a + b = {a + b}\")\n",
    "print(f\"a * b = {a * b}\")\n",
    "print(f\"a ** 2 = {a ** 2}\")\n",
    "print(f\"torch.exp(a) = {torch.exp(a)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Produit matriciel\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "# Trois façons équivalentes\n",
    "C1 = A @ B\n",
    "C2 = torch.matmul(A, B)\n",
    "C3 = A.mm(B)\n",
    "\n",
    "print(f\"A: {A.shape} @ B: {B.shape} = C: {C1.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting\n",
    "\n",
    "Le broadcasting permet d'opérer sur des tenseurs de formes différentes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Broadcasting\n",
    "X = torch.randn(32, 10)  # Batch de 32 vecteurs de dim 10\n",
    "bias = torch.randn(10)   # Un seul vecteur de dim 10\n",
    "\n",
    "# Le bias est ajouté à chaque ligne de X\n",
    "result = X + bias\n",
    "print(f\"X: {X.shape} + bias: {bias.shape} = {result.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1.2 : Implémentez une couche linéaire manuellement\n",
    "\n",
    "Une couche linéaire calcule : `y = x @ W.T + b`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def linear_layer(x, W, b):\n",
    "    \"\"\"\n",
    "    Implémente une couche linéaire.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor de shape (batch_size, in_features)\n",
    "        W: Poids de shape (out_features, in_features)\n",
    "        b: Biais de shape (out_features,)\n",
    "    \n",
    "    Returns:\n",
    "        Output tensor de shape (batch_size, out_features)\n",
    "    \"\"\"\n",
    "    # TODO: Implémenter\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "x = torch.randn(32, 10)  # batch_size=32, in_features=10\n",
    "W = torch.randn(5, 10)   # out_features=5, in_features=10\n",
    "b = torch.randn(5)       # out_features=5\n",
    "\n",
    "# y = linear_layer(x, W, b)\n",
    "# assert y.shape == (32, 5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autograd : Différentiation Automatique\n",
    "\n",
    "PyTorch peut calculer automatiquement les gradients via `autograd`.\n",
    "\n",
    "Comparez avec micrograd :\n",
    "- micrograd : `Value` avec `.backward()` scalaire\n",
    "- PyTorch : `Tensor` avec `.backward()` tensoriel"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# requires_grad=True active le suivi des gradients\n",
    "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
    "\n",
    "# Opération\n",
    "y = x ** 2\n",
    "z = y.sum()  # On doit réduire à un scalaire pour .backward()\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = x² = {y}\")\n",
    "print(f\"z = sum(y) = {z}\")\n",
    "\n",
    "# Calcul des gradients\n",
    "z.backward()\n",
    "\n",
    "# dz/dx = 2x\n",
    "print(f\"\\nGradient de z par rapport à x:\")\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"Attendu: 2*x = {2*x.detach()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2.1 : Vérifier le gradient\n",
    "\n",
    "Pour `f(x) = sigmoid(x) = 1 / (1 + exp(-x))`, vérifiez que le gradient calculé par PyTorch est correct.\n",
    "\n",
    "Rappel : `d/dx sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Exercice 2.1\n",
    "x = torch.tensor([0.0, 1.0, -1.0], requires_grad=True)\n",
    "\n",
    "# Calculer sigmoid(x)\n",
    "# y = ???\n",
    "\n",
    "# Calculer le gradient\n",
    "# ???\n",
    "\n",
    "# Vérifier avec la formule analytique\n",
    "# expected_grad = ???"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. nn.Module : Construire des Réseaux\n",
    "\n",
    "PyTorch fournit `nn.Module` comme classe de base pour les réseaux de neurones."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Couche linéaire intégrée\n",
    "linear = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "print(f\"Poids: {linear.weight.shape}\")\n",
    "print(f\"Biais: {linear.bias.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(32, 10)\n",
    "y = linear(x)\n",
    "print(f\"\\nInput: {x.shape} -> Output: {y.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Réseau simple avec nn.Sequential\n",
    "simple_mlp = nn.Sequential(\n",
    "    nn.Linear(10, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "print(simple_mlp)\n",
    "\n",
    "# Compter les paramètres\n",
    "n_params = sum(p.numel() for p in simple_mlp.parameters())\n",
    "print(f\"\\nNombre de paramètres: {n_params:,}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3.1 : Créer un MLP personnalisé\n",
    "\n",
    "Créez une classe `MLP` qui hérite de `nn.Module` avec :\n",
    "- Une couche d'entrée\n",
    "- N couches cachées avec ReLU\n",
    "- Une couche de sortie"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension d'entrée\n",
    "            hidden_dims: Liste des dimensions cachées [64, 32, 16]\n",
    "            output_dim: Dimension de sortie\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Construire le réseau\n",
    "        # Hint: utilisez nn.ModuleList ou construisez une liste de couches\n",
    "        # puis utilisez nn.Sequential\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implémenter le forward pass\n",
    "        pass\n",
    "\n",
    "# Test\n",
    "# model = MLP(input_dim=10, hidden_dims=[64, 32], output_dim=1)\n",
    "# x = torch.randn(32, 10)\n",
    "# y = model(x)\n",
    "# assert y.shape == (32, 1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entraînement : La Boucle Complète\n",
    "\n",
    "Voici les étapes d'une boucle d'entraînement typique :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Données synthétiques pour l'exemple\n",
    "X = torch.randn(100, 10)\n",
    "y = (X[:, 0] + X[:, 1] > 0).float().unsqueeze(1)\n",
    "\n",
    "# Modèle\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1)\n",
    ")\n",
    "\n",
    "# Fonction de perte\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "losses = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    # 1. Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "    \n",
    "    # 2. Backward pass\n",
    "    optimizer.zero_grad()  # Reset des gradients\n",
    "    loss.backward()        # Calcul des gradients\n",
    "    \n",
    "    # 3. Mise à jour des poids\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        acc = ((torch.sigmoid(outputs) > 0.5) == y).float().mean()\n",
    "        print(f\"Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Acc: {acc:.2%}\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Courbe d\\'apprentissage')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fonctions de Perte\n",
    "\n",
    "PyTorch fournit plusieurs fonctions de perte :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Régression\n",
    "mse_loss = nn.MSELoss()\n",
    "y_pred = torch.tensor([2.5, 0.0, 2.1])\n",
    "y_true = torch.tensor([3.0, -0.5, 2.0])\n",
    "print(f\"MSE Loss: {mse_loss(y_pred, y_true):.4f}\")\n",
    "\n",
    "# Classification binaire (avec logits)\n",
    "bce_loss = nn.BCEWithLogitsLoss()\n",
    "logits = torch.tensor([0.5, -1.0, 2.0])\n",
    "labels = torch.tensor([1.0, 0.0, 1.0])\n",
    "print(f\"BCE Loss: {bce_loss(logits, labels):.4f}\")\n",
    "\n",
    "# Classification multi-classe\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "logits = torch.tensor([[2.0, 1.0, 0.1], [0.5, 2.0, 0.3]])  # 2 samples, 3 classes\n",
    "labels = torch.tensor([0, 1])  # Classes correctes\n",
    "print(f\"CrossEntropy Loss: {ce_loss(logits, labels):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimiseurs : SGD vs Adam\n",
    "\n",
    "Comparons les deux optimiseurs les plus courants :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Fonction à minimiser : f(x) = x^2 + y^2\n",
    "def rosenbrock(params):\n",
    "    x, y = params\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def train_optimizer(opt_class, lr, n_steps=500, **kwargs):\n",
    "    params = torch.tensor([0.0, 0.0], requires_grad=True)\n",
    "    optimizer = opt_class([params], lr=lr, **kwargs)\n",
    "    \n",
    "    history = []\n",
    "    for _ in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = rosenbrock(params)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(params.detach().clone().numpy())\n",
    "    \n",
    "    return np.array(history)\n",
    "\n",
    "# Comparer SGD et Adam\n",
    "sgd_history = train_optimizer(optim.SGD, lr=0.001)\n",
    "adam_history = train_optimizer(optim.Adam, lr=0.01)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(sgd_history[:, 0], sgd_history[:, 1], 'b-', alpha=0.7, label='SGD')\n",
    "plt.plot(adam_history[:, 0], adam_history[:, 1], 'r-', alpha=0.7, label='Adam')\n",
    "plt.scatter([1], [1], c='green', s=100, marker='*', label='Optimum')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Trajectoire d\\'optimisation')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sgd_losses = [rosenbrock(torch.tensor(p)).item() for p in sgd_history]\n",
    "adam_losses = [rosenbrock(torch.tensor(p)).item() for p in adam_history]\n",
    "plt.semilogy(sgd_losses, label='SGD')\n",
    "plt.semilogy(adam_losses, label='Adam')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss (log)')\n",
    "plt.legend()\n",
    "plt.title('Convergence')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Régularisation\n",
    "\n",
    "Techniques pour éviter l'overfitting :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "dropout = nn.Dropout(p=0.3)  # 30% des neurones désactivés\n",
    "\n",
    "x = torch.ones(1, 10)\n",
    "\n",
    "# En mode training\n",
    "dropout.train()\n",
    "print(f\"Train mode: {dropout(x)}\")\n",
    "\n",
    "# En mode eval (pas de dropout)\n",
    "dropout.eval()\n",
    "print(f\"Eval mode: {dropout(x)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Weight Decay (L2 regularization)\n",
    "model = nn.Linear(10, 1)\n",
    "\n",
    "# Sans weight decay\n",
    "opt_no_wd = optim.Adam(model.parameters(), lr=0.01, weight_decay=0)\n",
    "\n",
    "# Avec weight decay\n",
    "opt_with_wd = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01)\n",
    "\n",
    "print(\"Weight decay pénalise les gros poids pour encourager des solutions plus simples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercice Final : Préparez-vous pour le Tournoi !\n",
    "\n",
    "Maintenant, vous avez tous les outils pour participer au Tournoi de la Guilde.\n",
    "\n",
    "Modifiez le modèle `baseline_model.py` et `train.py` pour améliorer les performances !"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Charger les données du tournoi\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "print(f\"Dataset d'entraînement: {len(train_df)} aventuriers\")\n",
    "print(f\"\\nFeatures:\")\n",
    "print(train_df.columns.tolist())\n",
    "print(f\"\\nStatistiques:\")\n",
    "train_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Votre modèle amélioré ici !\n",
    "class ImprovedGuildOracle(nn.Module):\n",
    "    def __init__(self, input_dim=12):\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Votre architecture améliorée\n",
    "        # Pensez à:\n",
    "        # - Dropout\n",
    "        # - BatchNorm\n",
    "        # - Architecture plus simple\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récapitulatif\n",
    "\n",
    "| Concept | micrograd | PyTorch |\n",
    "|---------|-----------|----------|\n",
    "| Données | `Value` scalaire | `Tensor` N-dim |\n",
    "| Gradients | `.backward()` manuel | `autograd` automatique |\n",
    "| Couches | Classes custom | `nn.Module` |\n",
    "| Optimisation | Boucle manuelle | `optim.Adam/SGD` |\n",
    "\n",
    "**Prochaine étape** : Le Tournoi de la Guilde !"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
